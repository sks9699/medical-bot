{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK!\n"
     ]
    }
   ],
   "source": [
    "print(\"OK!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Monu kumar\\OneDrive\\Desktop\\End-to-end-Medical-Chatbot-using-Llama2\\venv\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "# PINECONE_API_KEY = \"8d8905b7-a0c9-4765-b687-639f837aa7dd\"\n",
    "# # PINECONE_API_ENV = \"gcp-starter\"\n",
    "pc = Pinecone(api_key=\"8d8905b7-a0c9-4765-b687-639f837aa7dd\")\n",
    "index = pc.Index(\"medical-bot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract data from the PDF\n",
    "def load_pdf(data):\n",
    "    loader = DirectoryLoader(data,\n",
    "                    glob=\"*.pdf\",\n",
    "                    loader_cls=PyPDFLoader)\n",
    "    \n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract data from the PDF\n",
    "def load_pdf(data):\n",
    "    loader = DirectoryLoader(data,\n",
    "                    glob=\"*.pdf\",\n",
    "                    loader_cls=PyPDFLoader)\n",
    "    \n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_pdf(\"Data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create text chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 20)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make text chunks\n",
    "text_chunks = text_split(extracted_data)\n",
    "#text chunks iterable and metadatalist\n",
    "text = [t.page_content for t in text_chunks]\n",
    "metadata_list = [{\"text\": chunk.page_content} for chunk in text_chunks]\n",
    "\n",
    "# print(\"length of my chunk:\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#download embedding model\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "query_result = model.embed_query(\"Hello world\")\n",
    "print(\"Length\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='mation presented in this publication, the Gale Group neither guaranteesthe accuracy of the data contained herein nor assumes any responsibili-ty for errors, omissions or discrepancies. The Gale Group accepts nopayment for listing, and inclusion in the publication of any organiza-tion, agency, institution, publication, service, or individual does notimply endorsement of the editor or publisher. Errors brought to theattention of the publisher and verified to the satisfaction of the publish-er', metadata={'source': 'Data\\\\Medical_book.pdf', 'page': 3})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_list = [model.embed_query(text) for text in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pinecone import Pinecone done\n",
    "\n",
    "# Initialize Pinecone instance\n",
    "# pc = Pinecone(api_key=\"8d8905b7-a0c9-4765-b687-639f837aa7dd\")\n",
    "# index = pc.Index(\"medical-bot\") done\n",
    "\n",
    "# Extract text content from text chunks\n",
    "# text_content_list = [t.page_content for t in text_chunks]\n",
    "# metadata_list = [{\"text\": chunk.page_content} for chunk in text_chunks]done\n",
    "\n",
    "# Generate embeddings for the text chunks\n",
    "# # model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\") done\n",
    "# index_name=\"medical-bot\"\n",
    "\n",
    "# # Create vectors\n",
    "# vectors = []\n",
    "# for i, emb in enumerate(embeddings_list):\n",
    "#     vector = {\n",
    "#         \"id\": f\"vec{i}\",\n",
    "#         \"values\": emb , # Convert embedding tuple to list\n",
    "#         \"metadata\": metadata_list[i]\n",
    "#     }\n",
    "#     vectors.append(vector)\n",
    "# batch_size=100\n",
    "# # Upsert vectors in batches\n",
    "# for i in range(0, len(vectors), batch_size):\n",
    "#     batch = vectors[i:i+batch_size]\n",
    "#     index.upsert(batch, index_name=index_name, namespace=\"ns1\")\n",
    "# index.upsert(vectors, index_name=index_name, namespace=\"ns1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GALE ENCYCLOPEDIA OF MEDICINE 2 117Allergies\n",
      "Allergic rhinitis is commonly triggered by\n",
      "exposure to household dust, animal fur,or pollen. The foreign substance thattriggers an allergic reaction is calledan allergen.\n",
      "The presence of an allergen causes the\n",
      "body's lymphocytes to begin producingIgE antibodies. The lymphocytes of an allergy sufferer produce an unusuallylarge amount of IgE.\n",
      "IgE molecules attach to mast\n",
      "cells, which contain histamine.HistaminePollen grains\n",
      "Lymphocyte\n",
      "FIRST EXPOSURE\n"
     ]
    }
   ],
   "source": [
    "# #If we already have an index we can load it like this\n",
    "# docsearch=Pinecone.from_existing_index(index_name, embeddings)\n",
    "# from pinecone import Pinecone\n",
    "query_result = model.embed_query(\"What are Allergies\")\n",
    "\n",
    "# docs=docsearch.similarity_search(query, k=3)\n",
    "\n",
    "# print(\"Result\", docs)\n",
    "# pc = Pinecone(api_key=\"8d8905b7-a0c9-4765-b687-639f837aa7dd\")\n",
    "# index = pc.Index(\"medical-bot\")\n",
    "\n",
    "result = index.query(\n",
    "    namespace=\"ns1\",\n",
    "    vector=query_result,\n",
    "    top_k=3,  # Set top_k to 1 to retrieve only the top 1 match\n",
    "    include_values=True,\n",
    "    include_metadata=True,\n",
    ")\n",
    "result\n",
    "# Extract text from the top match (if any)\n",
    "if result['matches']:  # Check if there are any matches returned\n",
    "    top_match = result['matches'][0]  # Access the first (top) match\n",
    "    top_text = top_match['metadata']['text']\n",
    "   \n",
    "    print(top_text)\n",
    "else:\n",
    "    print(\"No matches found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=\"\"\"\n",
    "Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "# chain_type_kwargs={\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\l'\n",
      "C:\\Users\\Monu kumar\\AppData\\Local\\Temp\\ipykernel_16184\\3597584327.py:1: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  llm=CTransformers(model=\"model\\llama-2-7b-chat.ggmlv3.q4_0.bin\",\n"
     ]
    }
   ],
   "source": [
    "llm=CTransformers(model=\"model\\llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "                  model_type=\"llama\",\n",
    "                  config={'max_new_tokens':512,\n",
    "                          'temperature':0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.vectorstores import Pinecone\n",
    "# index = pc.Index(\"medical-bot\")\n",
    "\n",
    "# text_field = \"text\"\n",
    "# vectorstore = Pinecone(\n",
    "#     index, model.embed_query,text_field\n",
    "# from langchain.vectorstores import Pinecone\n",
    "# index = pc.Index(\"medical-bot\")\n",
    "\n",
    "# text_field = \"text\"\n",
    "# vectorstore = Pinecone(\n",
    "#     index, model.embed_query,text_field\n",
    "# )\n",
    "from langchain.vectorstores import Pinecone \n",
    "text_field = \"text\"\n",
    "vectorstore = Pinecone(\n",
    "    index=index,\n",
    "    embedding_function=model.embed_query,\n",
    "    text_key=text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qa=RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=vectorstore.as_retriever(top_k=2),\n",
    "    return_source_documents=True, \n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     user_input = input(\"Input Prompt: \")\n",
    "#     query_vector = model.embed_query(user_input)  # Embed the user input into a vector\n",
    "#     result = index.query(\n",
    "#         vector=query_vector,  # Provide vector as a keyword argument\n",
    "#         top_k=2,  # Specify top_k as a keyword argument\n",
    "#         namespace='ns1',  # Specify namespace as a keyword argument\n",
    "#         include_values=True,\n",
    "#         include_metadata=True,\n",
    "#     )\n",
    "# if result['matches']:  # Check if there are any matches returned\n",
    "#     top_match = result['matches'][0]  # Access the first (top) match\n",
    "#     top_text = top_match['metadata']['text']\n",
    "   \n",
    "#     print(top_text)\n",
    "# else:\n",
    "#     print(\"No matches found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     user_input=input(f\"Input Prompt:\")\n",
    "#     result=index.query(vector=model.embed_query(user_input), top_k=2, namespace='ns1')\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`run` not supported when there is not exactly one output key. Got ['result', 'source_documents'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      2\u001b[0m     user_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput Prompt:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     result\u001b[38;5;241m=\u001b[39m\u001b[43mqa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse : \u001b[39m\u001b[38;5;124m\"\u001b[39m, result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Monu kumar\\OneDrive\\Desktop\\End-to-end-Medical-Chatbot-using-Llama2\\venv\\Lib\\site-packages\\langchain\\chains\\base.py:310\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run the chain as text in, text out or multiple variables, text out.\"\"\"\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;66;03m# Run at start to make sure this is possible/defined\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m _output_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_output_key\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs:\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Monu kumar\\OneDrive\\Desktop\\End-to-end-Medical-Chatbot-using-Llama2\\venv\\Lib\\site-packages\\langchain\\chains\\base.py:294\u001b[0m, in \u001b[0;36mChain._run_output_key\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_output_key\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    295\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` not supported when there is not exactly \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    296\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mone output key. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    297\u001b[0m         )\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: `run` not supported when there is not exactly one output key. Got ['result', 'source_documents']."
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input=input(f\"Input Prompt:\")\n",
    "    result=qa.invoke({\"input\": user_input})\n",
    "    print(\"Response : \", result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mchatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
